{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546974c4",
   "metadata": {},
   "source": [
    "# 0 : Importation des librairies\n",
    "\n",
    "Pour ce projet, j'ai besoin d'importer : \n",
    "- librairies classiques de manipulations de données (**_pandas, numpy, scipy, sklearn_**)\n",
    "- fonctions du dossier **_functions_**\n",
    "- librairies classiques d'affichage (**_seaborn, matplotlib_**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c22c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Importation des librairies classiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Importation des modèles d'imputations ML\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # nécessaire\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Importation des fonctions du fichier functions/processing_functions.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from functions.processing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839284d2",
   "metadata": {},
   "source": [
    "# 0.bis : Importation des fichiers\n",
    "\n",
    "Le challenge ayant déja pris soin de séparer le dataset en un dataset d'entraînement et un dataset de test, je vais simplement fusionner les datasets **_x_train_** et **_y_train_** via la colonne **_DELIVERY_START_** préalablement lue comme une date. Cela me permettra de travailler ce dataset comme un seul et unique dataset et plus facilement récuperer les relations entre variables explicatives et cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('../data/raw/X_train_Wwou3IE.csv', parse_dates = ['DELIVERY_START']) # 10605 rows x 10 columns\n",
    "y_train = pd.read_csv('../data/raw/y_train_jJtXgMX.csv', parse_dates = ['DELIVERY_START']) # 10605 rows x 2 columns\n",
    "\n",
    "x_test = pd.read_csv('../data/raw/X_test_GgyECq8.csv', parse_dates = ['DELIVERY_START']) # 4942 rows x 10 columns\n",
    "\n",
    "# Passage \n",
    "for df in [x_train, y_train, x_test]:\n",
    "    df['DELIVERY_START'] = (\n",
    "        pd.to_datetime(df['DELIVERY_START'], utc=True) # Convertit 03:30:00+02:00 en 01:30:00+00:00 UTC\n",
    "        .dt.tz_convert(\"Europe/Paris\") # Convertit 01:30:00+00:00 UTC en 03:30:00+02:00 Fuseau Paris\n",
    "        .dt.tz_localize(None) # Supprime la notion de Fuseau, résultat : 03:30:00\n",
    "    )\n",
    "\n",
    "# Jointure de x_train et y_train sur 'DELIVERY_START'\n",
    "df_train = pd.merge(x_train, y_train, on = 'DELIVERY_START') # 10605 rows x 11 columns\n",
    "\n",
    "print(df_train['DELIVERY_START'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ed4b2",
   "metadata": {},
   "source": [
    "# 1.a : Atomisation des données\n",
    "\n",
    "Le marché Spot est un marché détérminant un prix pour chaque heure de la journée, il est donc crucial d'isoler cette information. Aussi, ce marché doit à tout prix faire converger l'offre et la demande à tout instant, il faut donc être en mesure de prédire au plus proche la consommation en électricité. Cette consommation dépend de différentes tendances à différentes échelles. On consomme plus en hier qu'en été, le matin et le soir qu'en milieu d'après-midi ou encore les jours de la semaine plutot que le week-end. Ainsi, il semble pertinent d'extraire et d'encoder toutes ces informations à partir de la variable **_DELIVERY_START_**.\n",
    "\n",
    "La première étape de processing de ce projet consiste en l'atomisation des informations contenues dans **_DELIVERY_START_** en les informations suivantes : \n",
    "\n",
    "- **_hour_** : variable entière de 0 à 23 \n",
    "- **_is_peak_hour_** : variable binaire _(1 = peak_hour, 2 = off_peak)_\n",
    "- **_is_weekend_** : variable binaire _(1 = week-end, 2 = week-day)_\n",
    "- **_day_of_week_** : variable entière de 1 à 7\n",
    "- **_month_** : variable entière de 1 à 12\n",
    "- **_season_** : variable entière : 1 = 'winter' ; 2 = 'spring' ; 3 = 'summer' ; 4 = 'autumn' (ca permettra au modèle de tourner, on pourra mettre en place ce labeling)\n",
    "\n",
    "Dans un contexte de prédiction des heures de pointes, je considère qu'il vaut mieux avoir trop d'heure de pointes que pas assez et risquer de manquer de prédire une tension sur le marché et ainsi perdre beaucoup d'argent voire être sanctionné d'une pénalité sur le marché Intraday.\n",
    "\n",
    "Dans cette objectif d'exploiter au mieux les informations fournies par ce dataset et dans un contexte de marché de l'énergie, il semble pertinent d'ajouter quelques variables explicatives.\n",
    "\n",
    "J'introduis donc **_dispatchable_capacity_**, représentant la capacité totale pilotable (charbon, gaz, nucléaire), utile pour évaluer la capacité de réponse à la demande, **_load_vs_dispatchable_** qui construit le ratio entre demande et capacité pilotable et permet d'indiquer les potentielles tensions sur le marché. \n",
    "\n",
    "J'ajoute aussi **_renewable_share_**, mesurant la part des énergies renouvelables, afin d’anticiper les effets du merit order, où une forte disponibilité renouvelable tend à faire baisser les prix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique la fonction extract_time_features implémentées dans le fichier functions.processing\n",
    "print(df_train['DELIVERY_START'].dtype)\n",
    "df_train = extract_time_features(df_train)\n",
    "df_train = add_energy_features(df_train)\n",
    "\n",
    "print(\"df_train :\", df_train.shape)\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0d31f",
   "metadata": {},
   "source": [
    "Visuellement, il est clair qu'il y'a deux blocs de missing values à traiter. D'une part, toutes les données de consommation ont été perdue entre le 01/07/2022 et le 22/08/2022, ceci correspond donc à 1262 lignes inutilisables tant l'information **_load_forecast_** est cruciale. \n",
    "\n",
    "Aussi, il semble inconsidérer de tenter une imputation via un KNN ou une régression classique qui viendrait ajouter énormément de bruit pour nos modèles ultérieures, étant donné que ce sont des données temporelles qui se suivent _(difficile d'interpoler la 650ème missing value du bloc sans les valeurs qui la précédent et la suivent)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(index=range(4176, 5439)).reset_index(drop=True)\n",
    "\n",
    "# Affichage de la heatmap des missing values\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df_train.isnull(), cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"HeatMap des missing values (df_train)\", fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
